{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 rows saved to preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "original_csv = 'Reviews.csv'\n",
    "\n",
    "# Read the first 1000 rows of the CSV file\n",
    "df = pd.read_csv(original_csv, nrows=5000)\n",
    "\n",
    "# Define the new CSV file name\n",
    "new_csv = 'preprocessed.csv'\n",
    "\n",
    "# Save the first 1000 rows to a new CSV file\n",
    "df.to_csv(new_csv, index=False)\n",
    "\n",
    "print(\"First 1000 rows saved to\", new_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Initialize GPT-2 tokenizer and model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_reviews = pd.read_csv('preprocessed.csv')\n",
    "train_data, test_data = train_test_split(preprocessed_reviews, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_column='Text', block_size=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_column = text_column\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.data[self.text_column].iloc[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            article,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.block_size,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "def train(train_data, model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    # Set the padding token to the end-of-sequence token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    train_dataset = CustomTextDataset(train_data, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=5e-6,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#train_file_path = \"train_data.csv\"  # Path to your training data CSV file\n",
    "model_name = \"gpt2\"  # Name of the pretrained model\n",
    "output_dir = \"./gpt2_finetuned\"  # Output directory for saving the finetuned model\n",
    "overwrite_output_dir = True  # Whether to overwrite the output directory if it already exists\n",
    "per_device_train_batch_size = 8  # Batch size per GPU/CPU during training\n",
    "num_train_epochs = 10  # Number of training epochs\n",
    "save_steps = 10000  # Save checkpoint every specified number of steps during training\n",
    "\n",
    "train_data = train_data  # Load your training data\n",
    "train(train_data, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"/home/mk/Desktop/IR_ass_4/gpt2_finetuned\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Example review text\n",
    "example_review_text = \"\"\n",
    "# Example reference summary\n",
    "example_reference_summary = \"\"\n",
    "\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    max_length = len(inputs[0]) + 10  # Adjust max_length to be greater than or equal to the length of the input sequence\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate summary for the example review text\n",
    "example_generated_summary = generate_summary(example_review_text)\n",
    "\n",
    "# Print example summaries\n",
    "print(\"Given Review Text:\", example_review_text)\n",
    "print(\"Given Summary:\", example_reference_summary)\n",
    "print(\"Generated Summary:\", example_generated_summary)\n",
    "\n",
    "# Initialize the Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Example ROUGE score calculation\n",
    "rouge_scores = rouge.get_scores(example_generated_summary, example_reference_summary)\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric, scores in rouge_scores[0].items():\n",
    "    print(f\"ROUGE-{metric}: Precision: {scores['p']:.2f}, Recall: {scores['r']:.2f}, F1-Score: {scores['f']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data.to_csv('summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"/home/mk/Desktop/IR_ass_4/gpt2_finetuned\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Define the function to generate summaries\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    max_length = len(inputs[0]) + 10  # Adjust max_length to be greater than or equal to the length of the input sequence\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Initialize the Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Example function to calculate ROUGE scores for a DataFrame\n",
    "def calculate_rouge_scores(df, generated_column, reference_column):\n",
    "    rouge_scores = []\n",
    "    for idx, row in df.iterrows():\n",
    "        generated_summary = str(row[generated_column]).strip()  # Remove leading/trailing whitespaces\n",
    "        reference_summary = str(row[reference_column]).strip()  # Remove leading/trailing whitespaces\n",
    "        if generated_summary and reference_summary:  # Check if neither the generated nor reference summary is empty\n",
    "            scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "            rouge_scores.append(scores)\n",
    "    return rouge_scores\n",
    "\n",
    "# Assuming you have a DataFrame df with generated summaries in \"Generated_Summary\" column\n",
    "# and reference summaries in \"Summary\" column\n",
    "test_data.to_csv('summary.csv', index=False)\n",
    "test_data = pd.read_csv('summary.csv')  # Assuming you have test data in a CSV file\n",
    "test_data[\"Generated_Summary\"] = test_data[\"Text\"].apply(lambda row: generate_summary(row))\n",
    "test_data.to_csv('summary.csv', index=False)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores(test_data, \"Generated_Summary\", \"Summary\")\n",
    "\n",
    "# Print the ROUGE scores\n",
    "for scores in rouge_scores:\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1:\n",
      "Precision: 0.03, Recall: 0.23, F1-Score: 0.05\n",
      "\n",
      "Average ROUGE-2:\n",
      "Precision: 0.00, Recall: 0.06, F1-Score: 0.01\n",
      "\n",
      "Average ROUGE-l:\n",
      "Precision: 0.02, Recall: 0.22, F1-Score: 0.04\n",
      "\n",
      "Average ROUGE Scores (Combined):\n",
      "Average ROUGE-1 score: 0.045087922268717655\n",
      "Average ROUGE-2 score: 0.009015151774630099\n",
      "Average ROUGE-L score: 0.04400684118763658\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_rouge_scores(rouge_scores):\n",
    "    rouge_1_precision = []\n",
    "    rouge_1_recall = []\n",
    "    rouge_1_f1 = []\n",
    "\n",
    "    rouge_2_precision = []\n",
    "    rouge_2_recall = []\n",
    "    rouge_2_f1 = []\n",
    "\n",
    "    rouge_l_precision = []\n",
    "    rouge_l_recall = []\n",
    "    rouge_l_f1 = []\n",
    "\n",
    "    for scores in rouge_scores:\n",
    "        # ROUGE-1\n",
    "        rouge_1_precision.append(scores[0]['rouge-1']['p'])\n",
    "        rouge_1_recall.append(scores[0]['rouge-1']['r'])\n",
    "        rouge_1_f1.append(scores[0]['rouge-1']['f'])\n",
    "\n",
    "        # ROUGE-2\n",
    "        rouge_2_precision.append(scores[0]['rouge-2']['p'])\n",
    "        rouge_2_recall.append(scores[0]['rouge-2']['r'])\n",
    "        rouge_2_f1.append(scores[0]['rouge-2']['f'])\n",
    "\n",
    "        # ROUGE-l\n",
    "        rouge_l_precision.append(scores[0]['rouge-l']['p'])\n",
    "        rouge_l_recall.append(scores[0]['rouge-l']['r'])\n",
    "        rouge_l_f1.append(scores[0]['rouge-l']['f'])\n",
    "\n",
    "    # Calculate average precision, recall, and F1-score for each ROUGE component\n",
    "    avg_rouge_1_precision = sum(rouge_1_precision) / len(rouge_1_precision)\n",
    "    avg_rouge_1_recall = sum(rouge_1_recall) / len(rouge_1_recall)\n",
    "    avg_rouge_1_f1 = sum(rouge_1_f1) / len(rouge_1_f1)\n",
    "\n",
    "    avg_rouge_2_precision = sum(rouge_2_precision) / len(rouge_2_precision)\n",
    "    avg_rouge_2_recall = sum(rouge_2_recall) / len(rouge_2_recall)\n",
    "    avg_rouge_2_f1 = sum(rouge_2_f1) / len(rouge_2_f1)\n",
    "\n",
    "    avg_rouge_l_precision = sum(rouge_l_precision) / len(rouge_l_precision)\n",
    "    avg_rouge_l_recall = sum(rouge_l_recall) / len(rouge_l_recall)\n",
    "    avg_rouge_l_f1 = sum(rouge_l_f1) / len(rouge_l_f1)\n",
    "\n",
    "    return {\n",
    "        'rouge-1': {'precision': avg_rouge_1_precision, 'recall': avg_rouge_1_recall, 'f1': avg_rouge_1_f1},\n",
    "        'rouge-2': {'precision': avg_rouge_2_precision, 'recall': avg_rouge_2_recall, 'f1': avg_rouge_2_f1},\n",
    "        'rouge-l': {'precision': avg_rouge_l_precision, 'recall': avg_rouge_l_recall, 'f1': avg_rouge_l_f1},\n",
    "        'average': {\n",
    "            'rouge-1': avg_rouge_1_f1,\n",
    "            'rouge-2': avg_rouge_2_f1,\n",
    "            'rouge-l': avg_rouge_l_f1\n",
    "        }\n",
    "    }\n",
    "# Example usage:\n",
    "average_rouge_scores = calculate_average_rouge_scores(rouge_scores)\n",
    "\n",
    "# Print the average ROUGE scores\n",
    "print(\"Average ROUGE-1:\")\n",
    "print(f\"Precision: {average_rouge_scores['rouge-1']['precision']:.2f}, \"\n",
    "      f\"Recall: {average_rouge_scores['rouge-1']['recall']:.2f}, \"\n",
    "      f\"F1-Score: {average_rouge_scores['rouge-1']['f1']:.2f}\")\n",
    "\n",
    "print(\"\\nAverage ROUGE-2:\")\n",
    "print(f\"Precision: {average_rouge_scores['rouge-2']['precision']:.2f}, \"\n",
    "      f\"Recall: {average_rouge_scores['rouge-2']['recall']:.2f}, \"\n",
    "      f\"F1-Score: {average_rouge_scores['rouge-2']['f1']:.2f}\")\n",
    "\n",
    "print(\"\\nAverage ROUGE-l:\")\n",
    "print(f\"Precision: {average_rouge_scores['rouge-l']['precision']:.2f}, \"\n",
    "      f\"Recall: {average_rouge_scores['rouge-l']['recall']:.2f}, \"\n",
    "      f\"F1-Score: {average_rouge_scores['rouge-l']['f1']:.2f}\")\n",
    "\n",
    "# Print the average ROUGE scores in a combined format\n",
    "print(\"\\nAverage ROUGE Scores (Combined):\")\n",
    "print(\"Average ROUGE-1 score:\", average_rouge_scores['average']['rouge-1'])\n",
    "print(\"Average ROUGE-2 score:\", average_rouge_scores['average']['rouge-2'])\n",
    "print(\"Average ROUGE-L score:\", average_rouge_scores['average']['rouge-l'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
